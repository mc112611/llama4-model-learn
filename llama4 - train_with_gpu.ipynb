{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "executionInfo": {
     "elapsed": 8848,
     "status": "ok",
     "timestamp": 1746490670603,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "86d72aab"
   },
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1746490670605,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "595b7bc3"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Llama4TextConfig:\n",
    "    vocab_size: int = 202048  # 词表大小（包含所有token的数量）\n",
    "    hidden_size: int = 5120  # 隐藏层维度（也即token embedding的维度）\n",
    "    intermediate_size: int = 8192  # 注意力模块中的中间维度（如用于RMSNorm等）\n",
    "    intermediate_size_mlp: int = 16384  # MLP模块的中间层维度\n",
    "    num_hidden_layers: int = 48  # Transformer block 的层数\n",
    "    num_attention_heads: int = 40  # 注意力头的数量\n",
    "    num_key_value_heads: int = 8  # KV-head 的数量（可小于attention head数量，表示采用multi-query attention）\n",
    "    head_dim: int = 128  # 每个注意力头的维度\n",
    "    max_position_embeddings: int = 4096 * 32  # 最大位置编码长度（支持的最大token序列长度）\n",
    "    rms_norm_eps: float = 1e-5  # RMSNorm中的epsilon值，防止除以0\n",
    "    pad_token_id: int = 200018  # padding token 的ID\n",
    "    bos_token_id: int = 1  # 序列开始（Begin Of Sequence）token 的ID\n",
    "    eos_token_id: int = 2  # 序列结束（End Of Sequence）token 的ID\n",
    "    rope_theta: float = 500000  # RoPE位置编码中的 theta 参数（值越大，支持的序列长度越长）\n",
    "    attention_dropout: float = 0.0  # 注意力模块中的dropout比例（通常在inference时为0）\n",
    "    num_experts_per_tok: int = 1  # 每个token选中的专家数量（MoE中 Top-K 的K值）\n",
    "    num_local_experts: int = 16  # 每个MoE层中包含的专家数量\n",
    "    use_qk_norm: bool = True  # 是否对 QK 做归一化处理（影响注意力权重分布）\n",
    "    no_rope_layer_interval: int = 4  # 每隔多少层不使用RoPE（控制位置编码插入的层间距）\n",
    "    attention_chunk_size: int = 8192  # 用于大模型推理时的注意力分块大小（节省显存）\n",
    "    attn_temperature_tuning: float = 4  # 注意力温度调整因子，用于调节softmax的分布平滑程度\n",
    "    floor_scale: int = 8192  # 温度缩放的最小尺度（影响softmax的归一化）\n",
    "    attn_scale: float = 0.1  # 注意力得分的缩放因子，通常用于调节softmax前的qk dot product结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "8e84c14a"
   },
   "source": [
    "## 构造MOE模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "f79a194f"
   },
   "source": [
    "### 创建MOE的专家对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1746490670606,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "8c0ad750"
   },
   "outputs": [],
   "source": [
    "class Llama4TextExperts(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Llama4TextExperts, self).__init__()\n",
    "\n",
    "        # 专家数量\n",
    "        self.num_experts = config.num_local_experts\n",
    "        # 前馈全连接中间层维度大小\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        # 隐藏层维度\n",
    "        self.hidden_size = config.hidden_size\n",
    "        # 专家维度，与FFN层维度统一\n",
    "        self.expert_dim = config.intermediate_size\n",
    "\n",
    "        # 下面三行是MOE的MLP部分，用来实现激活函数前后的线性变换，由于激活函数是将线型变为非线性。\n",
    "        # gate_up_proj是一个输入数据的加工（专业说法叫上投影层），用nn.Parameter告诉pytorch这是一个可以训练的权重，创建一个没有初始化数值的张量，形状为：[专家数量,隐藏层维度，2倍的专家嵌入维度]\n",
    "        # 之所以是2倍，因为这一层，我们需要计算两个不同的输出向量：一个是“值（value）”，一个是“门（gate）”。其中gate用于判断，value用于计算。\n",
    "        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2*self.expert_dim))\n",
    "        # down_proj是下投影层，用于把上投影层加工的结果变回原始维度的大小，方便后续处理\n",
    "        self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.expert_dim, self.hidden_size))\n",
    "        # 一个激活函数，门控结构，用于过滤，让输出更加平滑。也用于线型神经网络变为非线性，能够处理复杂问题\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "        # 对上投影层和下投影层做初始化，使用正态分布。\n",
    "        nn.init.normal_(self.gate_up_proj)\n",
    "        nn.init.normal_(self.down_proj)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "\n",
    "        # 原作者注释：\n",
    "        ### Hidden States we Pass in Here are already Sorted (according to the logic explained in Llama4TextMoe) ###\n",
    "        ### Go Ahead and Split the Num Experts and Num Tokens Dimension Up (Num Experts x Num Tokens x Embed Dim) ###\n",
    "\n",
    "        # 个人注释：\n",
    "        # 调整数据形状变为：(num_experts, num_tokens, hidden_size)\n",
    "        hidden_states = hidden_states.reshape(self.num_experts, -1, self.hidden_size)\n",
    "\n",
    "        #原作者注释：\n",
    "        ### Now Multiply All our Tokens (copied for each Expert) by each Expert Embeddings ###\n",
    "        ### gate_up_proj: (Num Experts x Hidden Size x 2*Expert Dim) ###\n",
    "        ### gate_up -> (Num Experts x Num Tokens x 2*Expert Dim)\n",
    "\n",
    "        # 使用“数据加工——gate_up_proj” 对当前输入的隐藏层状态进行“加工”，加工方法为批矩阵乘法\n",
    "        gate_up = torch.bmm(hidden_states, self.gate_up_proj)\n",
    "        # 原作者注释：\n",
    "        ### The reason our linear layer was 2*Expert Dim output was because we do a gated linear layer\n",
    "        ### just like you find in previous Llama implementations\n",
    "        ### First lets go ahead and chunk on the final dimension so we get two tensors:\n",
    "        ### gate -> (Num Experts x Num Tokens x Expert Dim)\n",
    "        ### up -> (Num Experts x Num Tokens x Expert Dim)\n",
    "\n",
    "        #个人注释：\n",
    "        # 将gate_up矩阵，在最后一个维度拆分成2份。  之前的 2*self.expert_dim ，现在拆成2等份，每一份最后一个维度为expert_dim。对应gate和value\n",
    "        gate, up = gate_up.chunk(2, dim=-1)\n",
    "\n",
    "        # 原作者注释：\n",
    "        ### Apply the Activation Function to Up (the SiLU activation) and multiply to gate\n",
    "        ### Effectively giving finer control over what information continues and what doesnt\n",
    "        ### gated -> (Num Experts x Num Tokens x Expert Dim)\n",
    "\n",
    "        # 个人注释：\n",
    "        # 变量gate是门控信息，up里是实际内容。    这里是先将gate信息经过激活函数变为非线性，然后按元素与up中的值做乘法。\n",
    "        gated = up * self.act_fn(gate)\n",
    "        # 原作者注释：\n",
    "        ### Now Finally Projec the Expert Dim back down to the hidden size using Down Proj ###\n",
    "\n",
    "        #个人注释：\n",
    "        # 用下投影层将处理过的内容还原，方便后续处理，执行分块矩阵相乘，这样原始数据张量便带有了专家门控信息。\n",
    "        next_states = torch.bmm(gated, self.down_proj)\n",
    "        # 原作者注释：\n",
    "        ### Flatten from (Num Experts x Num Tokens x Embed Dim) -> (Num Experts*Num Tokens x Embed Dim) ###\n",
    "\n",
    "        #个人注释：\n",
    "        # 原作者注释写的很清楚，改变最终单个专家输出的形状：(Num Experts x Num Tokens x Embed Dim) -> (Num Experts*Num Tokens x Embed Dim)\n",
    "        next_states = next_states.view(-1,self.hidden_size)\n",
    "\n",
    "        return next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1746490670668,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "146a9ab9"
   },
   "outputs": [],
   "source": [
    "# LLaMa的共享专家层。减小计算开销，对输入进行统一处理。\n",
    "# 共享专家负责统一处理所有的输入，执行固定的计算：门控线性变换 -> 上投影 -> 非线性激活 ->下投影\n",
    "# 共享专家输出的数据形状，与输入是一样的\n",
    "class Llama4TextMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        \"\"\"\n",
    "        Exactly the same as the Llama4TextExperts, but this is a single MLP layer. In\n",
    "        Llama4 they pass tokens to both an MOE and a single MLP and add the results\n",
    "        \"\"\"\n",
    "\n",
    "        super(Llama4TextMLP, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.activation_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gated = self.activation_fn(self.gate_proj(x)) * self.up_proj(x)\n",
    "        return self.down_proj(gated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1746490670679,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "8fc68704"
   },
   "outputs": [],
   "source": [
    "# 构建LLama4的MOE层\n",
    "class Llama4TextMoe(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Llama4TextMoe, self).__init__()\n",
    "\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.experts = Llama4TextExperts(config)\n",
    "        self.router = nn.Linear(config.hidden_size, config.num_local_experts, bias=False)\n",
    "        self.shared_expert = Llama4TextMLP(config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # [batch_size, seq_len, embed_dim]，比如是 [2, 128, 4096] 表示有两个句子，每个句子 128 个词，每个词用 4096 维的向量表示。\n",
    "        batch_size, seq_len, embed_dim = hidden_states.shape\n",
    "\n",
    "        ### Flatten Batch and Seq_Len Dimensions so we have (Num Tokens x Embed Dim) ###\n",
    "        # 转换数据形状，将输入数据的隐藏层向量变为 ：Num Tokens x Embed Dim\n",
    "        hidden_states = hidden_states.view(-1, self.hidden_dim)\n",
    "\n",
    "        ### Get Expert Index for Each Token (Num Tokens x Num Experts) ###\n",
    "        # 用一个线性层，对输入的隐藏层数据进行打分，确定当前输入的数据更适合哪个“专家”，线性层输出为：专家数量num_local_experts\n",
    "        router_logits = self.router(hidden_states)    # 路由器选择专家\n",
    "\n",
    "        ### Now we want each token to go to its own expert, but to parallize this easily we can do the following:\n",
    "        ### Step 1: Lets pretend each expert will get ALL TOKENS ###\n",
    "        # 第一步：假设每个专家都会获得所有的token，全部token数tokens_per_expert\n",
    "        tokens_per_expert = batch_size * seq_len\n",
    "\n",
    "        ### Step 2: Get the topK experts for each token (from our router logits)\n",
    "        ### this gives:\n",
    "        ###     router_top_value: The logit from our router logits sorted for each token from largest to smallest (upto k of them)\n",
    "        ###     router_indices: The indexes of which expert had the largest logit to smallest (upto k of them)\n",
    "\n",
    "        # 将路由器输出的结果，即16个专家，选择top_k个进行激活，输出结果为：router_top_value代表最大得分的数值，router_indices最大得分对应的是哪两个专家的索引\n",
    "        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=1)\n",
    "\n",
    "        ### Step 3: Create a Matrix of -infinity in the shape of (Num Tokens x Num Experts)\n",
    "        ### Fill this matrix at the indexes of the topk experts selected by the router for each token with the logits for those k experts\n",
    "        ### Basically makes sure we have -inf for the non topk tokens and its router logit otherwise!\n",
    "        ### And go ahead and transpose this matrix so it finally becomes as (Num Experts x Num Tokens)\n",
    "\n",
    "        # 创建一个和router_logits一样大的矩阵，内容填充为负无穷，表示：默认所有专家都不被选中，使用scatter将矩阵中的部分位置，替换成 Top-K 专家的分数。索引为router_indices，值为router_top_value\n",
    "        # 之后进行一次转置，因为当前是每一列代表一个序列。我们将其转置变为每一行为一个句子序列，代表每个token的专家得分\n",
    "        router_scores = torch.full_like(router_logits, float(\"-inf\")).scatter_(dim=1, index=router_indices, src=router_top_value).transpose(0,1)\n",
    "\n",
    "        ### Step 4: Because we are passing in \"ALL TOKENS\" to every expert, lets update our router indicies to be\n",
    "        ### indexes from 0 to NUM TOKENS, repeated for EVERY Expert! It will something like:\n",
    "\n",
    "        ### [0, 1, 2, ..., Num Tokens]\n",
    "        ### [0, 1, 2, ..., Num Tokens]\n",
    "        ### [0, 1, 2, ..., Num Tokens]\n",
    "        ### ...\n",
    "\n",
    "        ### Repeating the number of rows for the number of experts we have!\n",
    "\n",
    "        # 首先创建一个一维数组，用来表示每个token的编号，类似于：[0, 1, 2, ..., num_tokens - 1] ，tokens_per_expert = batch_size * seq_len，就是 token 的总数量\n",
    "        # 把一维数组变为二维：[[0, 1, 2, ..., num_tokens - 1]]\n",
    "        # 让这个数组复制 num_experts 行.\n",
    "        # 因为我们下一步要把 hidden_states 中所有 token 的 embedding，复制一份给每个专家。N 个 token，每个专家都要获得这 N 个 token 的编号，用来复制取出数据。\n",
    "\n",
    "        router_indices = torch.arange(tokens_per_expert, device=hidden_states.device).unsqueeze(0).expand(router_scores.size(0), -1)\n",
    "\n",
    "        ### Step 5: Now when we grab our embeddings with these indexes, we have the embedding dimension as well. Our Data is the shape\n",
    "        ### of (Num Tokens x Embed Dim) Lets go ahead and flatten our router indicies, add a dimension for the embed dim and repeat.\n",
    "        ### This means we will end with a (Num_Experts*Num_tokens x Embed Dim) Matrix at the end.\n",
    "\n",
    "        # 之前我们拿到二维的token索引矩阵：[num_experts, num_tokens]\n",
    "        # 现在，每一个 token 的索引，要扩展成 [hidden_dim] 个维度，用来一次性 gather 所有 embedding 维度上的数据\n",
    "        # 我们需要让 router_indices 的形状变成 [num_experts * num_tokens, hidden_dim]\n",
    "        router_indices = router_indices.reshape(-1,1).expand(-1, self.hidden_dim)\n",
    "\n",
    "        ### Step 6: Update our Router Scores to be between 0 and 1. All our non topk scores (that are currently -inf) will become 0!\n",
    "        # 将router_scores中的值经过sigmoid激活函数，负无穷变为0，其他值变为0~1之间的值，再将张量统一一下浮点数精度。\n",
    "        router_scores = torch.sigmoid(router_scores.float()).to(hidden_states.dtype)\n",
    "\n",
    "        ### Step 7: Gather All Our Hidden States By our router_indices (repeating all tokens for each Expert!)\n",
    "\n",
    "        # gather是一个根据索引去获取值的函数，需要学习一下。在这里，主要是通过index，专家路由器给出的索引，在input（hidden_states）的第一个维度，取数据。\n",
    "        # 相当于，所有的专家通过这一步才是拿到token的内容。\n",
    "        routed_in = torch.gather(\n",
    "            input=hidden_states,\n",
    "            dim=0,\n",
    "            index=router_indices\n",
    "        )\n",
    "\n",
    "        ### Step 8: We now have repeated all our token embeddings for every expert. But we only want to keep the experts that\n",
    "        ### were in our TopK. Well, we hav eour router_scores that is 0 for non Topk expert indicies. We can therefore go\n",
    "        ### ahead and multiply, as it will 0 out our embeddings assigned to a an expert that was not in its TopK\n",
    "        ### This also multiplies the embeddings from their topk experts by the weights assigned by the router\n",
    "\n",
    "        ### router_scores -> (Num Experts x Num Tokens)\n",
    "        ### routed_in -> (Num Experts*Num Tokens x Embed Dim)\n",
    "\n",
    "        # routed_in形状：[num_experts * num_tokens, hidden_dim]\n",
    "        # 将router_scores的形状，从[num_experts, num_tokens] 变为 [num_experts * num_tokens, 1]\n",
    "        # 然后按元素相乘（加权）\n",
    "        # 最终结果是对routed_in的内容，保留重要的token信息，抑制不重要的信息\n",
    "        routed_in = routed_in * router_scores.reshape(-1, 1)\n",
    "\n",
    "        ### Lets Pass this to our Experts!! ###\n",
    "        ### routed_out -> (Num_Experts*Num_Tokens x Embed Dim)\n",
    "\n",
    "        # 将处理好的数据，输入进专家。\n",
    "        # 当routed_in输入进专家模型，发生了什么：self.experts 将根据每个 token 对应的专家模型进行处理。每个专家（专家模型）会处理它接收到的 token 数据，产生一个输出。这些专家是并行的，处理不同 token 数据时具有不同的能力和权重。\n",
    "        # routed_out 是一个新的 tensor，包含了经过所有专家模型处理后的 token 数据。它的形状通常是 [num_experts * num_tokens, hidden_dim]，和 routed_in 一样，因为每个 token 对应多个专家输出\n",
    "        routed_out = self.experts(routed_in)\n",
    "\n",
    "        ### Similary, pass our Hidden States to the Shared Expert ###\n",
    "        ### In Llama4 we have both Experts and a shared expert ###\n",
    "        ### and we add the results together at the end! ###\n",
    "        ### shared_expert_out -> (Num_Tokens x Embed Dim)\n",
    "\n",
    "        # 经过共享专家MLP层处理\n",
    "        # 这里hidden_states的形状为：[batch_size * seq_len, hidden_dim]     是将batch_size中所有token合并成一个大的token表。每个token包含embedding信息。\n",
    "        shared_expert_out = self.shared_expert(hidden_states)\n",
    "\n",
    "        ### Now we Add our Results Together! But we have a bit of a challenge ###\n",
    "        ### routed_out repeats all tokens for all experts (but we zeroed out the experts that were not topk for each token)\n",
    "        ### shared_expert_out is just our projection for all the tokens.\n",
    "        ### If we want to add our shared_expert_out to our routed_outs, we just need to grab the correct indicies from our\n",
    "        ### routed_outs that coorespond to the correct experts for each token, and then add it all together!\n",
    "\n",
    "        ### REMEMBER: The output of Tokens (when passed to their Non-Topk expert) will just be 0!\n",
    "        ### We are basically going through every experts output here and adding their outputs each expert\n",
    "        ### to our shared_expert_out. But because there are lots of zeros in our embeddings from each\n",
    "        ### expert (if that was not a topk expert), we are basically accumulating across all the tokens\n",
    "\n",
    "        ### This is the same as just for looping through all the expert outputs and adding it to our\n",
    "        ### shared_expert_out, but this is a oneliner that does that same thing!\n",
    "\n",
    "        ### Take a look at scatter_add_ if you want more details!\n",
    "        ### https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_\n",
    "\n",
    "\n",
    "\n",
    "        # shared_expert_out -> [num_tokens, hidden_dim]\n",
    "        # routed_out -> [num_experts * num_tokens, hidden_dim]\n",
    "        # 我们要把每个专家处理出来的结果（routed_out），累加 到对应的 token 上，也就是 shared_expert_out[token_idx] += routed_out[i]\n",
    "        # 用for循环是这样写：\n",
    "        # for i in range(routed_out.size(0)):\n",
    "        #     idx = router_indices[i]\n",
    "        #     shared_expert_out[idx] += routed_out[i]\n",
    "\n",
    "        # 假设每个专家有自己擅长的领域，router会给每个token分配到top_k个专家，每个专家都会返回一个向量（代表每个专家的理解），将这些专家给出的向量结果累加起来，就是输入内容的最终向量表示。\n",
    "        # 因为每个专家擅长领域不同，并且只选top_k个专家去处理，会漏掉一部分信息，而共享专家会将所有内容进行计算，因此共享专家用来修正top_k个专家给出的结果。因此要加上共享专家输出的结果。\n",
    "        shared_expert_out.scatter_add_(dim=0, index=router_indices, src=routed_out)\n",
    "\n",
    "        return shared_expert_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "c2e5dfd2"
   },
   "source": [
    "## 旋转位置编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746490670680,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "b2a93122"
   },
   "outputs": [],
   "source": [
    "class Llama4TextRotaryEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, config, device=None):\n",
    "        super(Llama4TextRotaryEmbedding, self).__init__()\n",
    "\n",
    "        # 保存最大支持的位置长度   KVcache\n",
    "        self.max_seq_len_cached = config.max_position_embeddings\n",
    "        # 初始最大序列长度\n",
    "        self.original_max_seq_len = config.max_position_embeddings\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # 旋转位置编码器所需要的频率值\n",
    "        inv_freq = self._compute_default_rope_parameters(self.config, device=device)\n",
    "        # 将这个频率值注册进模型结构中，而不是作为一个变量。\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    # 计算旋转角频率\n",
    "    def _compute_default_rope_parameters(self, config, device):\n",
    "\n",
    "        base = config.rope_theta\n",
    "        head_dim = config.head_dim\n",
    "\n",
    "\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / head_dim))\n",
    "\n",
    "        return inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "\n",
    "        ### x is our data (Batch x Seq Len x ) ###\n",
    "        ### position_ids is the position index of every token (Batch x Seq Len) ###\n",
    "\n",
    "        ### Our inv_freq is a vector of length (Hidden Dim/2). Lets add dimensions and repeat batch size number of times\n",
    "        ### inv_freq_expanded -> (Batch x Hidden_Dim/2 x 1)\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "\n",
    "        ### Our position ids is just Batch x Seq Len, add a new dimension to make it (Batch x 1 x Seq Len)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        ### Now We can compute our freqs for all positions (by multiplying by the position index) ###\n",
    "        ### (B x H/2 X 1) @ (B x 1 x L) -> (B x H/2 x L)\n",
    "\n",
    "        with torch.autocast(device_type=x.device.type, enabled=False):\n",
    "            # 计算旋转角度（采样频率），广播成(B x H/2 x L)的形状\n",
    "            freqs = (inv_freq_expanded @ position_ids_expanded)\n",
    "\n",
    "            ### Now make sure Freqs is (B x L x H/2) by transpose ###\n",
    "            # 转置成 (B x L x H/2) 的形状\n",
    "            freqs = freqs.transpose(1,2)\n",
    "\n",
    "            ### Convert Frequencies to complex (polar) representations (with magnitude 1)###\n",
    "            # 生成复数形式的旋转位置编码\n",
    "            freqs_cis = torch.polar(abs=torch.ones_like(freqs), angle=freqs)\n",
    "\n",
    "        return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746490670695,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "a61955cf"
   },
   "outputs": [],
   "source": [
    "# 将复数的旋转位置编码器，注入到q和k张量中\n",
    "def apply_rotary_emb(xq, xk, freqs_cis):\n",
    "\n",
    "    ### We want to \"rotate\" every consecutive pair along the embedding dimension ###\n",
    "    ### by our freqs_cis, Lets first split them up! ###\n",
    "    ### xq_split -> (B x Seq_len x Num Heads x Embed_dim/2 x 2)\n",
    "    ### xk_split -> (B x Seq_len x Num Heads x Embed_dim/2 x 2)\n",
    "\n",
    "    # 老规矩，将注意力机制的Q和K在最后一个维度进行分成两两一对儿。\n",
    "    # 假设xq原本为：xq.shape 形状为(2, 3, 4, 8)，对应(batch,seq_len,num_heads,dim)，变成(batch,seq_len,num_heads,dim/2,2)。也就是(2,3,4,4,2)\n",
    "    # 最后一个维度形状变化：[e0, e1, e2, e3, e4, e5, e6, e7] -> [e0, e1], [e2, e3], [e4, e5], [e6, e7]\n",
    "\n",
    "    xq_split = xq.float().reshape(*xq.shape[:-1], -1, 2)\n",
    "    xk_split = xk.float().reshape(*xk.shape[:-1], -1, 2)\n",
    "\n",
    "    ### Now rotating with a complex number is the same as mulitplying (phase shift) ###\n",
    "    ### take a look here for rotation property: https://wumbo.net/concepts/complex-number-system/ ###\n",
    "    ### technically multiplying also scales, but we made sure our freqs_cis was magnitude 1 ###\n",
    "    ### Problem: xq_split and xk_split are currently real numbers. Lets check what we wanted to do.\n",
    "    ### Lets go along the embedding dim for one token:\n",
    "    ### [e0, e1, e2, e3, e4, e5, e6, e7, e8, ...]\n",
    "    ### We have gone ahead and chunked it like the following:\n",
    "    ### [e0, e1]\n",
    "    ### [e2, e3]\n",
    "    ### [e4, e5]\n",
    "    ### [e6, e7]\n",
    "    ### ....\n",
    "\n",
    "    ### And now we want to rotate each of these 2dim vector by our complex freqs_cis. To do this correctly,\n",
    "    ### These 2dim vectors must also be complex so we can convert them to complex numbers like the following:\n",
    "    ### [e0+j*e1]\n",
    "    ### [e2+j*e3]\n",
    "    ### [e4+j*e5]\n",
    "    ### [e6+j*e7]\n",
    "    ### ....\n",
    "\n",
    "    ### xq_split -> (B x Seq_len x Num Heads x Embed_dim/2) # Complex number used the 2 dimension from earlier\n",
    "    ### xk_split -> (B x Seq_len x Num Heads x Embed_dim/2) # Complex number used the 2 dimension from earlier\n",
    "\n",
    "    # 将q和k转换成复数：[e0 + i·e1], [e2 + i·e3], ...\n",
    "    xq_split = torch.view_as_complex(xq_split)\n",
    "    xk_split = torch.view_as_complex(xk_split)\n",
    "\n",
    "    ### Now Go Ahead and Multiply with our freqs (adding extra dimension for attention heads) !\n",
    "    # 使用复数乘法，将q和k与之前的旋转位置编码进行按元素计算\n",
    "    xq_out = xq_split * freqs_cis[:, :, None, :]\n",
    "    xk_out = xk_split * freqs_cis[:, :, None, :]\n",
    "\n",
    "    ### Now that we have done our Rotary embeddings with complex numbers, go ahead and return back to real numbers ###\n",
    "    ### xq_out -> (B x Seq_len x Num Heads x Embed_dim/2 x 2) # Convert to real gave that 2 dimension back\n",
    "    ### xk_out -> (B x Seq_len x Num Heads x Embed_dim/2 x 2) # Convert to real gave that 2 dimension back\n",
    "\n",
    "    # 再把复数转换成普通实数。\n",
    "    xq_out = torch.view_as_real(xq_out)\n",
    "    xk_out = torch.view_as_real(xk_out)\n",
    "\n",
    "    ### Flatten Embed_dim/2 x 2 dimensions back to Embed_dim\n",
    "\n",
    "    # 还原之前的维度变换，也就是 （2，3，4，4，2） -> （2，3，4，8）\n",
    "    xq_out = xq_out.flatten(3)\n",
    "    xk_out =xk_out.flatten(3)\n",
    "\n",
    "    # 输出覆盖了旋转位置编码器的 q和k，并确保数据类型与输入相同。\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1746490670703,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "9aa051a9"
   },
   "outputs": [],
   "source": [
    "# 构造一个L2归一化的模块\n",
    "class Llama4TextL2Norm(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def _norm(self, x):\n",
    "\n",
    "        ### We want to do x / sqrt((x**2).mean()) along the last dimension! ###\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._norm(x.float()).type_as(x)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"eps={self.eps}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1746490670731,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "60206a74"
   },
   "outputs": [],
   "source": [
    "# 构造一个RMSNorm模块，输入为隐藏层维度\n",
    "class Llama4TextRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{tuple(self.weight.shape)}, eps={self.eps}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746490670745,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "a094865a"
   },
   "outputs": [],
   "source": [
    "# KV cache层\n",
    "class Cache:\n",
    "    \"\"\"\n",
    "    KV Cache Method that is close to the Huggingface DynamicCache\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/cache_utils.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        ### Counter for Number of Tokens in Cache ###\n",
    "        # 缓存的token数量（计数器）\n",
    "        self._seen_tokens = 0\n",
    "\n",
    "        ### Key/Value Cache (List of Tensor, where list is over model layers) ###\n",
    "        self.key_cache = [torch.tensor([]) for _ in range(config.num_hidden_layers)]\n",
    "        self.value_cache = [torch.tensor([]) for _ in range(config.num_hidden_layers)]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DyanmicCache(Num_Layers: {len(self.key_cache)} | Cached Tokens: {self.key_cache[0].shape[2]})\"\n",
    "\n",
    "    def update(self, key_states, value_states, layer_idx):\n",
    "\n",
    "        ### Only iterate num tokens seen on the first layer ###\n",
    "        ### key_states (B x H x L x E)\n",
    "        ### value_states (B x H x L x E)\n",
    "\n",
    "        # 形状为：(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        if layer_idx == 0:\n",
    "            # key_states的倒数第二个元素是seq_len\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        ### Append New key/Value states to key/value cache ###\n",
    "        # 将新的key_states和value_chache添加到缓存中。 torch.cat是沿着指定维度进行拼接。  这里是沿着seq_len进行拼接\n",
    "        self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "        self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "\n",
    "        # 返回更新后的缓存\n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "\n",
    "    # 该函数用于返回指定层ID的序列长度（seq_len），如果KVcache为空，则返回0\n",
    "    def get_seq_len(self, layer_idx=0):\n",
    "        return self.key_cache[layer_idx].shape[-2] if self.key_cache[layer_idx].numel() != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "d042cfd6"
   },
   "source": [
    "## LLama4注意力机制实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1746490670752,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "38610c9d",
    "outputId": "a5e71f6a-a86c-4eac-9159-945a3411066f"
   },
   "outputs": [],
   "source": [
    "a=Llama4TextConfig()\n",
    "print(dir(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1746490670784,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "c893d92b"
   },
   "outputs": [],
   "source": [
    "class Llama4TextAttention(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super(Llama4TextAttention, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = config.head_dim\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.scaling = self.head_dim **-0.5\n",
    "        self.attn_scale = config.attn_scale\n",
    "        self.floor_scale = config.floor_scale\n",
    "        self.attn_temperature_tuning = config.attn_temperature_tuning\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True\n",
    "        self.use_rope = int((layer_idx+1) % 4 == 0)\n",
    "\n",
    "        # 构造QKV的线型投影层。外加一个output的线型投影层\n",
    "        # 作用：把每个 token 的 hidden state 从维度 hidden_size 映射到 num_attention_heads * head_dim，然后 reshape 为 (B, L, H, D)。\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads*self.head_dim, bias=False)\n",
    "        # KV层因为共享缓存，因此假设num_key_value_heads为8，而head_dim为128，则K和V的维度为8*128=4096，节约计算成本\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads*self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads*self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.num_attention_heads*self.head_dim, config.hidden_size, bias=False)\n",
    "\n",
    "        if self.config.use_qk_norm and self.use_rope:\n",
    "            # 对Q和K执行L2归一化\n",
    "            self.qk_norm = Llama4TextL2Norm()\n",
    "        else:\n",
    "            self.qk_norm = None\n",
    "\n",
    "    def _repeat_kv(self, hidden_states, n_rep):\n",
    "\n",
    "        ### Add Extra Dimension to Repeat Over ###\n",
    "        ### (B x H x L x E) -> (B x H x 1 x L x E) -> (B x H x n_rep x L x E) -> (B x H*n_rep x L x E)\n",
    "\n",
    "        # 因为KV缓存加速计算，而KV的head数量只有8个，而Q使用的num_attention_heads有40个，因此，要对KV进行复制，40//8=5次\n",
    "        # 获取初始维度\n",
    "        batch, heads, seq_len, embed_dim = hidden_states.shape\n",
    "        #  expand: (B, H, n_rep, L, E) → 重复 n_rep 次，但不复制内存\n",
    "        hidden_states = hidden_states[:, :, None, :, :].expand(batch, heads, n_rep, seq_len, embed_dim)\n",
    "        # reshape：把 H 和 n_rep 合并，得到 (B, H * n_rep, L, E)\n",
    "        hidden_states = hidden_states.reshape(batch, heads*n_rep, seq_len, embed_dim)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states,\n",
    "                position_embeddings=None,\n",
    "                attention_mask=None,\n",
    "                past_key_value=None, # this is a Cache Object\n",
    "                cache_position=None):\n",
    "\n",
    "        ### Get Shape of Tensor (without Embed dims) ###\n",
    "        # 提取输入数据的除emb以外的形状\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "\n",
    "        ### Create a Shape Tuple (-1 for num heads in the future) ###\n",
    "        # 准备将数据变成(batch_size, seq_len, num_heads, head_dim)形状\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        ### Split Embed Dim by Heads ###\n",
    "        # 将输入数据拆分成QKV，并且形状与(batch_size, seq_len, num_heads, head_dim)一致\n",
    "        # 初始为(batch, seq_len, hidden_size)  ->  (batch_size, seq_len, num_heads, head_dim)，其中初始的hidden_size变为num_heads * head_dim\n",
    "        query_states = self.q_proj(hidden_states).reshape(hidden_shape)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape)\n",
    "        # 因为要做Q和K的矩阵乘法，因此要进行转置。\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1,2)\n",
    "\n",
    "        ### Apply RoPE to Query and Key ###\n",
    "        # 将旋转位置计算加入进attention\n",
    "        if self.use_rope:\n",
    "            query_states, key_states = apply_rotary_emb(query_states, key_states, position_embeddings.to(query_states.device))\n",
    "\n",
    "        ### L2 Norm ###\n",
    "        # 对Q和K进行L2归一化\n",
    "        # 这种方法来源于llama2\n",
    "        # 归一化后，Attention 得分只依赖于方向（角度），更像是余弦相似度。\n",
    "        if self.qk_norm is not None:\n",
    "            query_states = self.qk_norm(query_states)\n",
    "            key_states = self.qk_norm(key_states)\n",
    "\n",
    "        ### Temperature Tuning (on layers not using rope) ###\n",
    "        ### Not total sure how they came up with this as the scaling factor in the ###\n",
    "        ### Paper they referenced https://arxiv.org/abs/2501.19399\n",
    "        ### was just S log(N) where S was some learnable parameter\n",
    "\n",
    "        ### Here was their reasoning:\n",
    "        ### \"We are applying temperature tuning (https://arxiv.org/abs/2501.19399) to NoPE layers, where\n",
    "        ### the inference-time temperature tuning function is customized to not affect short context\n",
    "        ### while working at very long context\"\n",
    "\n",
    "        # 对Q的states进行温度调节\n",
    "        # 可以注意一下这个类的这个属性：self.use_rope = int((layer_idx+1) % 4 == 0) ,根据不同的layer_id，每4层应用一次ROPE。\n",
    "\n",
    "        # 以下是Nope的部分，即，不使用位置编码器。\n",
    "        # cache_position 是一个表示位置的张量，用来记录当前处理的 token 在序列中的位置。\n",
    "        # unsqueeze(0) 是在第一个维度上增加一个新的维度，让它变成二维张量，形状从 [seq_len] 变成 [1, seq_len]\n",
    "        # expand(hidden_states.shape[0], -1)：将 cache_position 张量在hidden_states的第一个维度进行扩展，即batch_size上进行扩展（重复），-1表示保持其他维度不变。\n",
    "        cache_position = cache_position.unsqueeze(0).expand(hidden_states.shape[0],-1)\n",
    "        # 当不使用rope时\n",
    "        if self.attn_temperature_tuning and not self.use_rope: # 这个参数不明白：attn_temperature_tuning\n",
    "            # 对于查询向量query_states进行温度调节\n",
    "            # 首先cache_position的形状是：(batch_size, seq_len)，转换成浮点数加一，相当于下标从1开始，然后除以温度缩放的最小尺度，对结果的每一个元素进行向下取整，结果再加一，乘以注意力缩放因子，再加一\n",
    "            # 这样得到的attn_scales会根据序列中token的不同位置，来调整注意力缩放因子。\n",
    "            # 个人觉得，这个地方绝对是一个优化的点。\n",
    "            attn_scales = (\n",
    "                torch.log(torch.floor((cache_position.float() + 1.0) / self.floor_scale) + 1.0) * self.attn_scale + 1.0\n",
    "            )\n",
    "\n",
    "            # 重塑attn_scales的形状，input_shape的形状为(batch_size, seq_len),转换成：(batch_size, seq_len，1，1)的形状\n",
    "            # 这样，attn_scales 就可以正确地与 query_states 中的每个 token 进行广播\n",
    "            attn_scales = attn_scales.view((*input_shape, 1, 1))\n",
    "            query_states = (query_states * attn_scales).to(query_states.dtype)\n",
    "\n",
    "        ### Flip the Head and Seq Len Dimensions ###\n",
    "        ### (B x L x H x D) -> (B x H x L x D)\n",
    "        # 将Q和K在第二，三维度进行转置，方便后续的计算\n",
    "        query_states = query_states.transpose(1,2)\n",
    "        key_states = key_states.transpose(1,2)\n",
    "\n",
    "        ### Update Key Value Cache ###\n",
    "        # past_key_value是一个缓存对象，需要更新key的状态向量和v的状态向量，以及神经网络层id\n",
    "        if past_key_value is not None:\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx)\n",
    "\n",
    "        ### Grouped Query Attention ###\n",
    "\n",
    "        ### Step 1: Repeat Keys/Values Heads to match the Query Heads\n",
    "        ### Query: (B x Attention Heads x L x E)\n",
    "        ### Key: (B x KV Heads x L x E)\n",
    "        ### Value: (B x KV Heads x L x E)\n",
    "\n",
    "        # 将K和V的头数与Q进行对齐，因此需要重复 config.num_attention_heads // config.num_key_value_heads次\n",
    "        key_states = self._repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = self._repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        ### Step 2: Compute Attention Weights ###\n",
    "        # Q和K矩阵做点积，再乘以缩放因子，根号下dk，self.scaling = self.head_dim **-0.5\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2,3)) * self.scaling\n",
    "\n",
    "        ### Step 3: Apply Attention Mask (add large negative numbers to masked positions) ###\n",
    "        ### Also we crop out attention mask only upto the tokens we need ###\n",
    "\n",
    "        # 注意力机制掩码，此处需要后面传入掩码矩阵\n",
    "        if attention_mask is not None:\n",
    "\n",
    "            ### Crop Attetion Mask to only include upto the number of key/value tokens we have to attend to ###\n",
    "            # key_states.shape[-2] 为seq_len ,在掩码矩阵的最后一个维度，选择序列长度的数值\n",
    "            causal_mask = attention_mask[:, :, :, :key_states.shape[-2]]\n",
    "\n",
    "            ### Add Causal Mask to Weights ###\n",
    "            # 将掩码矩阵和attn矩阵的元素按位置相加\n",
    "            attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        ### Standard Attention ###\n",
    "        # 执行注意力机制后面的计算：经过softmax层，再将结果与V做点积，得到的结果在第二、三维度变回原装，即：(batch_size, seq_len, num_heads, head_dim)\n",
    "        attn_weights = nn.functional.softmax(attn_weights.float(), dim=-1).to(query_states.dtype)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "        attn_output = attn_output.transpose(1,2)\n",
    "\n",
    "        ### Final Projection ###\n",
    "        # (batch_size, seq_len, num_heads, head_dim) 转换成为 (batch_size, seq_len, num_heads*head_dim)\n",
    "        attn_output = attn_output.flatten(2)\n",
    "        # 输出最终结果之前，经过线型层变换一下，多注意力头合并。\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1746490670832,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "b3144858",
    "outputId": "3f8fb73f-3df3-4a31-9b0c-de4bbfc29365"
   },
   "outputs": [],
   "source": [
    "a=Llama4TextConfig()\n",
    "print(dir(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746490670833,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "fb49f51e"
   },
   "outputs": [],
   "source": [
    "# 单个llamaBlock实现\n",
    "class Llama4TextDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super(Llama4TextDecoderLayer, self).__init__()\n",
    "\n",
    "        self.layer_idx = layer_idx\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = Llama4TextAttention(config, layer_idx)\n",
    "        self.use_chunked_attention = int((layer_idx+1) % 4 != 0)\n",
    "        self.feed_forward = Llama4TextMoe(config)\n",
    "        self.input_layernorm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states,\n",
    "                attention_mask,\n",
    "                chunk_causal_mask,\n",
    "                past_key_value,\n",
    "                cache_position,\n",
    "                position_embeddings):\n",
    "\n",
    "        residual = hidden_states\n",
    "\n",
    "        ### Normalize ###\n",
    "        # RMSNorm\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        ### Did you pass in chunked attention mask? Then use it if enabled! ###\n",
    "        # 是否使用分块注意力机制\n",
    "        if self.use_chunked_attention and chunk_causal_mask is not None:\n",
    "            attention_mask = chunk_causal_mask\n",
    "\n",
    "        ### Compute Attention ###\n",
    "        # 计算注意力机制\n",
    "        attention_states = self.self_attn(\n",
    "                hidden_states=hidden_states,\n",
    "                position_embeddings=position_embeddings,\n",
    "                attention_mask=attention_mask,\n",
    "                past_key_value=past_key_value, # this is a Cache Object\n",
    "                cache_position=cache_position\n",
    "        )\n",
    "\n",
    "        ### Residual Connection ###\n",
    "        # 将注意力机制计算的结果覆盖到原隐藏层向量矩阵，按元素相加。（残差连接）\n",
    "        hidden_states = residual + attention_states\n",
    "\n",
    "        ### MOE Layer ###\n",
    "        # 用MOE层替换传统的FFN层\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.feed_forward(hidden_states)\n",
    "        # 残差连接\n",
    "        hidden_states = residual + hidden_states.view(residual.shape)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1746490670891,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "02f610ca"
   },
   "outputs": [],
   "source": [
    "# 组装Llama的文本模型\n",
    "class Llama4TextModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Llama4TextModel, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        # <PAD> token的id\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        # 词表大小\n",
    "        self.vocab_sie = config.vocab_size\n",
    "\n",
    "        # 文本token嵌入，把词表中的词映射成固定长度向量\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=self.padding_idx)\n",
    "\n",
    "        # LlamaBlock的堆叠层数\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                Llama4TextDecoderLayer(config, layer_idx)\n",
    "                for layer_idx in range(config.num_hidden_layers)\n",
    "            ]\n",
    "        )\n",
    "        # 实例化RMS对象\n",
    "        self.norm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        # 实例化旋转位置编码器对象\n",
    "        self.rotary_emb = Llama4TextRotaryEmbedding(config)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                input_embeds=None,\n",
    "                attention_mask=None,\n",
    "                position_ids=None,\n",
    "                past_key_values=None,\n",
    "                cache_position=None):\n",
    "\n",
    "        ### Set up Key/Value Cache if it doesnt exist ###\n",
    "        # KVchche不存在，则创建一个对象\n",
    "        if past_key_values is None:\n",
    "            past_key_values = Cache(self.config)\n",
    "\n",
    "        ### Get Input Embeddings ###\n",
    "        # 将输入映射成为向量\n",
    "        if input_embeds is None:\n",
    "            hidden_states = self.embed_tokens(input_ids)\n",
    "        else:\n",
    "            hidden_states = input_embeds\n",
    "\n",
    "        ### Set up Cache Position ###\n",
    "        # 如果Nope的位置张量cache_position为空。\n",
    "        if cache_position is None:\n",
    "            # past_key_values 是缓存对象，它存储了之前步骤的键值对（key-value pairs）。get_seq_len() 获取缓存中已经处理过的 token 数量，表示已经处理过的序列长度。\n",
    "            past_seen_tokens = past_key_values.get_seq_len()\n",
    "            # 生成一个连续的整数序列，表示当前输入中每个 token 在整个序列中的位置\n",
    "            # 如果之前缓存的token为0，则生成seq_len长度的序列，作为nope的位置表示\n",
    "            cache_position = torch.arange(past_seen_tokens, past_seen_tokens+hidden_states.shape[1], device=hidden_states.device)\n",
    "\n",
    "        ### Setup Position Ids ###\n",
    "        if position_ids is None:\n",
    "            # cache_position的形状为(seq_len)，需要在第一个维度也就是0维度添加一个维度，变成（batch，seq_len）的形状\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        ### Update Causal Mask ###\n",
    "        # 注意力机制掩码的计算\n",
    "        causal_mask, chunked_attention_mask = self._update_causal_mask(\n",
    "            attention_mask, hidden_states, cache_position\n",
    "        )\n",
    "\n",
    "        ### Compute Rotary Embeddings ###\n",
    "        # 旋转位置编码的计算\n",
    "        freq_cis = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        ### Pass Through Layers ###\n",
    "        # LlamaBlock堆叠组成的model，对于每一个Block，需要将数据经过每一个Vlock前向传播进行计算，得到的隐藏层的向量，传递到下一层。\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states,\n",
    "                                  attention_mask,\n",
    "                                  chunked_attention_mask if chunked_attention_mask is not None else causal_mask,\n",
    "                                  past_key_values,\n",
    "                                  cache_position,\n",
    "                                  freq_cis)\n",
    "        # 当数据经过了所有的Block层，对隐藏层的状态向量进行归一化。\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # 返回前向传播的结果，以及用于减少计算量的kv缓存，用于下一次迭代不用重复计算。\n",
    "        return hidden_states, past_key_values\n",
    "\n",
    "\n",
    "    # 注意力的掩码张量计算包含了两个计算：一个是自回归的因果掩码，一个是分块掩码。\n",
    "    # 首先是因果掩码\n",
    "    def _prepare_4d_causal_attention_mask_with_cache_position(\n",
    "            self,\n",
    "            attention_mask,\n",
    "            sequence_length,\n",
    "            cache_position,\n",
    "            batch_size,\n",
    "            dtype,\n",
    "            device\n",
    "    ):\n",
    "        # 如果掩码张量已存在，则直接使用\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = attention_mask\n",
    "\n",
    "        else:\n",
    "\n",
    "            ### Create a Causal Mask ###\n",
    "            # torch.finfo(dtype).min是用来获取指定数据类型（dtype）的最小值\n",
    "            # 如：min_float32=-3.4028235e+38，而min_float64:-1.7976931348623157e+308\n",
    "            min_dtype = torch.finfo(dtype).min\n",
    "\n",
    "            # 创建一个形状为(seq*seq)大小的用最小值min_type填充的矩阵\n",
    "            # 为啥要用极小值？ 因为在做softmax的时候，如果用0填充，则softmax后，e^0 = 1，掩码的部分不会变小，而是获得更多的关注。\n",
    "            # 而使用极小值，会在经过softmax后，掩码的值更趋近于0\n",
    "            causal_mask = torch.full(\n",
    "                (sequence_length, sequence_length), fill_value=min_dtype, dtype=dtype, device=device\n",
    "            )\n",
    "\n",
    "            ### If we have more than one token, we need causal mask, so current token doesnt look forward ###\n",
    "            # 应用因果掩码，右上三角\n",
    "            if sequence_length != 1:\n",
    "                causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "\n",
    "            ### Check Wherever our Sequence index is longer than the cache positions ###\n",
    "            ### If a Query is BEFORE the Key/Value Cache Index, then the Query would have ###\n",
    "            ### to look into the future which is not allowed and needs to be masked! ###\n",
    "\n",
    "            # 作用，生成一个掩码，保证每个token只能看到自己之前的单词，没办法看到自己之后的单词。\n",
    "            # 这个代码的思路很有意思，生成seq_len长度的数，与位置缓存中的内容进行比对，返回True或False。\n",
    "            causal_mask *= torch.arange(sequence_length, device=device) > cache_position.to(device).reshape(-1,1)\n",
    "\n",
    "            ### Add Extra Dimension to causal Mask (4 dimensions, with placeholder for head dim) ###\n",
    "            # 将掩码张量添加batch_size，在第一维度，变为(batch_size, 1, sequence_length, sequence_length)\n",
    "            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "\n",
    "            ### If Attention mask is Not None, then add causal and attention mask together ###\n",
    "            if attention_mask is not None:\n",
    "                # 如果attention_mask不为空，则克隆一份作为causal_mask，原始causal_mask不被修改\n",
    "                causal_mask = causal_mask.clone()\n",
    "\n",
    "                ### Only keep upto length in attention mask ###\n",
    "                # 掩码的长度为最后一个维度\n",
    "                mask_length = attention_mask.shape[-1]\n",
    "                #causal_mask[:, :, :, :mask_length] 和 attention_mask[:, None, None, :] 进行了加法，生成了一个新的 padding_mask，它包含了 causal_mask 和 attention_mask 合并后的结果。\n",
    "                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(device)\n",
    "                # 生成bool值的掩码，当padding mask中的元素值为0的地方会被设置为True，否则为False\n",
    "                padding_mask = (padding_mask==0)\n",
    "                # masked_fill会将padding mask中为True的元素，使用极小值min dtype进行替换\n",
    "                causal_mask[:,:,:,:mask_length] = causal_mask[:,:,:,:mask_length].masked_fill(padding_mask, min_dtype)\n",
    "\n",
    "            return causal_mask\n",
    "    # 创建分块注意力掩码\n",
    "    def _create_chunked_attention_mask(\n",
    "            self,\n",
    "            attention_chunk_size,\n",
    "            start,\n",
    "            end,\n",
    "            device\n",
    "    ):\n",
    "\n",
    "        ### Create Blocks in the Chunk Sizes you want ###\n",
    "        # tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
    "        #         [0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
    "        #         [0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
    "        #         [1, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "        #         [1, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "        #         [1, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "        #         [2, 2, 2, 1, 1, 1, 0, 0, 0],\n",
    "        #         [2, 2, 2, 1, 1, 1, 0, 0, 0],\n",
    "        #         [2, 2, 2, 1, 1, 1, 0, 0, 0]])\n",
    "\n",
    "        # 按照块的大小attention_chunk_size进行创建掩码，创建两个长度相同的矩阵，分别在第1维度和第二维度添加新维度，分别与attention_chunk_size做整除，然后使用abs做两个张量之间差的绝对值\n",
    "        # 行向量和列向量通过广播机制进行相减，得到的结果为 (end-start)*(end-start)形状的矩阵\n",
    "        block_pos = torch.abs(\n",
    "            (torch.arange(start, end).unsqueeze(0)//attention_chunk_size) -\n",
    "            (torch.arange(start, end).unsqueeze(1)//attention_chunk_size)\n",
    "        )\n",
    "\n",
    "        ### Do computation again but without absolute value or division to get token positions###\n",
    "        # tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
    "        #         [-1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
    "        #         [-2, -1,  0,  1,  2,  3,  4,  5,  6],\n",
    "        #         [-3, -2, -1,  0,  1,  2,  3,  4,  5],\n",
    "        #         [-4, -3, -2, -1,  0,  1,  2,  3,  4],\n",
    "        #         [-5, -4, -3, -2, -1,  0,  1,  2,  3],\n",
    "        #         [-6, -5, -4, -3, -2, -1,  0,  1,  2],\n",
    "        #         [-7, -6, -5, -4, -3, -2, -1,  0,  1],\n",
    "        #         [-8, -7, -6, -5, -4, -3, -2, -1,  0]])\n",
    "\n",
    "        #构造一个二维的“位置差值矩阵”，行列相减，广播计算。\n",
    "        tok_pos = torch.arange(start, end).unsqueeze(0) - torch.arange(start,end).unsqueeze(1)\n",
    "\n",
    "        ### Wherever our block pos is 0 (our chunks) and our tok_pos is negative (causal mask) We want those ###\n",
    "        # 上面两个矩阵做运算，最终结果：\n",
    "        # tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                # [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                # [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "                # [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                # [0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "                # [0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "                # [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                # [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "                # [0, 0, 0, 0, 0, 0, 1, 1, 1]])\n",
    "\n",
    "        mask = (block_pos==0) & (tok_pos<=0)\n",
    "        # 此处计算，相当于既保留因果掩码，同时也使用分块注意力机制来减少计算量。\n",
    "        return mask.to(device)\n",
    "\n",
    "    def _update_causal_mask(self,\n",
    "                            attention_mask,\n",
    "                            input_tensor,\n",
    "                            cache_position):\n",
    "\n",
    "        ### Get Sequence Length and Attention Chunk Size ###\n",
    "        ### Honestly the attention chunk size is huge (over 8000)\n",
    "        ### we could probably skip this as we are just creating a\n",
    "        ### minimal version here but lets just do it anyway!\n",
    "\n",
    "        #序列长度\n",
    "        seq_len = input_tensor.shape[1]\n",
    "        # 获取分块注意力机制分多少块\n",
    "        attention_chunk_size = self.config.attention_chunk_size\n",
    "\n",
    "        ### Get the Starting and Ending Position from Cache ###\n",
    "        # 从位置信息缓存获取开始的token位置，默认batch_size为1\n",
    "        start_cache_pos = cache_position[0]\n",
    "\n",
    "        ### Two Checks: Is our First Cache Position greater than our chunk size:\n",
    "        # 确认当前是否超过attention_chunk_size，如果超过attention_chunk_size，代表不是第一个注意力掩码的分块，有可能是第二个或者更后面的chunk\n",
    "        cond1 = start_cache_pos >= attention_chunk_size\n",
    "\n",
    "        ### Is our first cache in the first chunk, but with the seq_len it rolls over to the second chunk ###\n",
    "        # 判断当前序列是否跨越了一个 attention chunk 的边界。这个条件的设计在分块注意力中非常关键，用于决定是否要做特殊的 mask 处理。\n",
    "        #\n",
    "        cond2 = (start_cache_pos < attention_chunk_size) & (start_cache_pos + seq_len > attention_chunk_size)\n",
    "\n",
    "        ### This is just a fancy if/else\n",
    "        ### If cond1 is True then key_length = attention_chunk_size + seq_len - 1\n",
    "        ### elif cond1 is false, then if cond2 is True then key_length = start_cache_pos + seq_len\n",
    "        ### else key_length = attention_chunk_size\n",
    "\n",
    "        # 当cond1 == True，表示整个序列起始位置就在 chunk 边界 之后，即你已经处在 chunk 1、chunk 2 等之后的块中。key 长度设为attention_chunk_size + seq_len - 1，key 范围涵盖前一个 chunk 全部，加上当前序列的长度，-1是为了对其index\n",
    "        # 当cond1 == False 且 cond2 == True，起始位置在第一个 chunk，但本次序列 跨越 chunk 边界。start_cache_pos + seq_len。表示从起始位置向前看，只允许看到从头到当前序列末尾的所有位置。\n",
    "        # 当cond1 == False 且 cond2 == False，key的长度为attention_chunk_size，表示 attention mask 只允许关注第一个 chunk 内的 token。\n",
    "        key_length = torch.where(\n",
    "            cond1,\n",
    "            attention_chunk_size + seq_len - 1,\n",
    "            torch.where(cond2, start_cache_pos + seq_len, attention_chunk_size),\n",
    "        )\n",
    "\n",
    "        # 对于短序列，采用完整的因果注意力机制掩码。\n",
    "        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n",
    "            attention_mask,\n",
    "            sequence_length=seq_len,\n",
    "            cache_position=cache_position,\n",
    "            batch_size=input_tensor.shape[0],\n",
    "            dtype=input_tensor.dtype,\n",
    "            device=input_tensor.device\n",
    "        )\n",
    "\n",
    "        # 对于长序列，使用分块注意力机制掩码，同时需要注意keylength的条件\n",
    "        chunked_attention_mask = None\n",
    "        if seq_len > self.config.attention_chunk_size:\n",
    "            chunked_attention_mask = self._create_chunked_attention_mask(\n",
    "                self.config.attention_chunk_size,\n",
    "                start=start_cache_pos,\n",
    "                end=start_cache_pos+key_length,\n",
    "                device=input_tensor.device\n",
    "            )\n",
    "\n",
    "            ### Mask only valid wherever attention mask is valid ###\n",
    "            # 避免 padding 位置参与 attention。\n",
    "            # 即使 chunked 允许 attend 某些位置，如果 attention_mask 说这些是 padding，也强行掩掉。\n",
    "            chunked_attention_mask = chunked_attention_mask & attention_mask\n",
    "\n",
    "            ### Add dimensions and fill with -inf ###\n",
    "\n",
    "            # 最终结果chunked_attention_mask是形状 [1, 1, T_q, T_k]、类型为 float16/32 的张量\n",
    "            chunked_attention_mask = (\n",
    "                    # 将 chunked_attention_mask的shape 从 [T_q, T_k] 扩展为 [1, 1, T_q, T_k]\n",
    "                    chunked_attention_mask[None, None, :, :]\n",
    "                    .to(input_tensor.dtype)\n",
    "                    # 掩码填充最小值，如果chunked_attention_mask元素为True则填充float的极小值作为掩码\n",
    "                    .masked_fill(chunked_attention_mask, torch.finfo(input_tensor.dtype).min)\n",
    "                )\n",
    "\n",
    "        return causal_mask, chunked_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1746490670895,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "c12c49dd"
   },
   "outputs": [],
   "source": [
    "# 组装Llama4的文本模型\n",
    "class Llama4ForCausalLM(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Llama4ForCausalLM, self).__init__()\n",
    "\n",
    "        self.model = Llama4TextModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                input_embeds=None,\n",
    "                attention_mask=None,\n",
    "                position_ids=None,\n",
    "                past_key_values=None):\n",
    "\n",
    "        outputs, past_key_values = self.model(\n",
    "            input_ids,\n",
    "            input_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "\n",
    "        logits = self.lm_head(outputs)\n",
    "\n",
    "        return logits, past_key_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "bdb6bf2a"
   },
   "source": [
    "## 测试一下Llama4的文本模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5110,
     "status": "ok",
     "timestamp": 1746490676006,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "3ffc31d1",
    "outputId": "348205ec-b930-48f1-c6de-4defc4c9ce41"
   },
   "outputs": [],
   "source": [
    "config=Llama4TextConfig(hidden_size=768,\n",
    "                        intermediate_size=768*4,\n",
    "                        intermediate_size_mlp=768*4,\n",
    "                        num_hidden_layers=2,\n",
    "                        num_attention_heads=12,\n",
    "                        num_key_value_heads=3\n",
    "                        )\n",
    "rope=Llama4TextRotaryEmbedding(config)\n",
    "rand=torch.randn(2,4,768,dtype=torch.float32)\n",
    "position_ids=torch.arange(0,4).unsqueeze(0).expand(2,-1)\n",
    "freqs_cis=rope(rand,position_ids)\n",
    "layer=Llama4TextModel(config)\n",
    "layer(input_embeds=rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "5e5f4b1e"
   },
   "source": [
    "## Llama4文本部分的训练，保存，以及推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1746490676494,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "66bfd3f7",
    "outputId": "2ba9dcac-cd27-4999-d787-ded748466ebe"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# 下载《西游记》文本\n",
    "url = \"https://raw.githubusercontent.com/mc112611/PI-ka-pi/main/xiyouji.txt\"\n",
    "file_name = \"xiyouji.txt\"\n",
    "urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1746490676741,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "2df711c7"
   },
   "outputs": [],
   "source": [
    "# 加载文本\n",
    "with open(file_name, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read()\n",
    "\n",
    "# 构建字符表\n",
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "\n",
    "# 编码函数\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(indices):\n",
    "    return ''.join([itos[i] for i in indices])\n",
    "\n",
    "# 转为 Tensor 数据集\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1746490676760,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "ff893c97"
   },
   "outputs": [],
   "source": [
    "# 修改一下参数，否则太大了\n",
    "\n",
    "config_test = Llama4TextConfig(\n",
    "    vocab_size=len(vocab),\n",
    "    hidden_size=256,\n",
    "    intermediate_size=512,\n",
    "    intermediate_size_mlp=512,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    num_key_value_heads=2,\n",
    "    head_dim=32,\n",
    "    max_position_embeddings=128 * 16,\n",
    "    rms_norm_eps=1e-5,\n",
    "    pad_token_id=1000,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    rope_theta=500000,\n",
    "    attention_dropout=0.0,\n",
    "    num_experts_per_tok=1,\n",
    "    num_local_experts=4,\n",
    "    use_qk_norm=True,\n",
    "    no_rope_layer_interval=4,\n",
    "    attention_chunk_size=512,\n",
    "    attn_temperature_tuning=4,\n",
    "    floor_scale=8192,\n",
    "    attn_scale=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1746490676768,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "04d85db9"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import urllib.request\n",
    "import random\n",
    "import os\n",
    "\n",
    "def create_dataloader(dataset, batch_size, context_window, split='train'):\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = int(0.1 * len(dataset))\n",
    "\n",
    "    if split == 'train':\n",
    "        data = dataset[:train_size]\n",
    "    elif split == 'val':\n",
    "        data = dataset[train_size:train_size + val_size]\n",
    "    else:\n",
    "        data = dataset[train_size + val_size:]\n",
    "\n",
    "    print(f\"Preparing {split} set with {len(data)} characters...\")\n",
    "\n",
    "    # tqdm 包裹 unfold 显示进度\n",
    "    data = tqdm(data.unfold(0, context_window, 1), desc=\"Sliding window\")\n",
    "    data = torch.stack(list(data))\n",
    "    return DataLoader(TensorDataset(data), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "executionInfo": {
     "elapsed": 4489,
     "status": "ok",
     "timestamp": 1746490681258,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "b607c6a6"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "embedding = nn.Embedding(config_test.vocab_size, config_test.hidden_size).to(device)\n",
    "model = Llama4TextModel(config_test).to(device)\n",
    "output_head = nn.Linear(config_test.hidden_size, config_test.vocab_size).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    list(model.parameters()) + list(embedding.parameters()) + list(output_head.parameters()), lr=1e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "263425f0"
   },
   "source": [
    "## GPU训练\n",
    "\n",
    "为了使用GPU进行训练，我们需要将KVcache，model，embedding放到显卡上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746490681262,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "6349cd35"
   },
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    \"\"\"\n",
    "    KV Cache Method that is close to the Huggingface DynamicCache\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/cache_utils.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        ### Counter for Number of Tokens in Cache ###\n",
    "        # 缓存的token数量（计数器）\n",
    "        self._seen_tokens = 0\n",
    "\n",
    "        ### Key/Value Cache (List of Tensor, where list is over model layers) ###\n",
    "        self.key_cache = [torch.tensor([]) for _ in range(config.num_hidden_layers)]\n",
    "        self.value_cache = [torch.tensor([]) for _ in range(config.num_hidden_layers)]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DyanmicCache(Num_Layers: {len(self.key_cache)} | Cached Tokens: {self.key_cache[0].shape[2]})\"\n",
    "\n",
    "    def update(self, key_states, value_states, layer_idx):\n",
    "        ### Only iterate num tokens seen on the first layer ###\n",
    "        ### key_states (B x H x L x E)\n",
    "        ### value_states (B x H x L x E)\n",
    "\n",
    "        if layer_idx == 0:\n",
    "            self._seen_tokens += key_states.shape[-2]\n",
    "\n",
    "        ### Append New key/Value states to key/value cache ###\n",
    "        self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
    "        self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
    "\n",
    "        # 返回更新后的缓存\n",
    "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
    "\n",
    "    def get_seq_len(self, layer_idx=0):\n",
    "        return self.key_cache[layer_idx].shape[-2] if self.key_cache[layer_idx].numel() != 0 else 0\n",
    "\n",
    "    # 新增：设备迁移方法\n",
    "    def to(self, device):\n",
    "        self.key_cache = [key.to(device) for key in self.key_cache]\n",
    "        self.value_cache = [value.to(device) for value in self.value_cache]\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1746490681302,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "b867f770"
   },
   "outputs": [],
   "source": [
    "# 组装Llama的文本模型\n",
    "class Llama4TextModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(Llama4TextModel, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        # <PAD> token的id\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        # 词表大小\n",
    "        self.vocab_sie = config.vocab_size\n",
    "\n",
    "        # 文本token嵌入，把词表中的词映射成固定长度向量\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=self.padding_idx)\n",
    "\n",
    "        # LlamaBlock的堆叠层数\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                Llama4TextDecoderLayer(config, layer_idx)\n",
    "                for layer_idx in range(config.num_hidden_layers)\n",
    "            ]\n",
    "        )\n",
    "        # 实例化RMS对象\n",
    "        self.norm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        # 实例化旋转位置编码器对象\n",
    "        self.rotary_emb = Llama4TextRotaryEmbedding(config)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                input_embeds=None,\n",
    "                attention_mask=None,\n",
    "                position_ids=None,\n",
    "                past_key_values=None,\n",
    "                cache_position=None):\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        ### Set up Key/Value Cache if it doesnt exist ###\n",
    "        if past_key_values is None:\n",
    "            # 确保 KVcache 在相同设备上\n",
    "            past_key_values = Cache(self.config).to(device)\n",
    "\n",
    "        ### Get Input Embeddings ###\n",
    "        if input_embeds is None:\n",
    "            # 确保 embedding 层输出在相同设备上\n",
    "            hidden_states = self.embed_tokens(input_ids).to(device)\n",
    "        else:\n",
    "            # 确保 input_embeds 在相同设备上\n",
    "            hidden_states = input_embeds.to(device)\n",
    "\n",
    "        ### Set up Cache Position ###\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_len()\n",
    "            cache_position = torch.arange(past_seen_tokens, past_seen_tokens+hidden_states.shape[1], device=hidden_states.device)\n",
    "            # 确保 cache_position 在相同设备上\n",
    "            cache_position = cache_position.to(device)\n",
    "\n",
    "        ### Setup Position Ids ###\n",
    "        # 确保 position_ids 在相同设备上\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0).to(device)\n",
    "\n",
    "        ### Update Causal Mask ###\n",
    "        causal_mask, chunked_attention_mask = self._update_causal_mask(\n",
    "            attention_mask, hidden_states, cache_position\n",
    "        )\n",
    "\n",
    "        ### Compute Rotary Embeddings ###\n",
    "        freq_cis = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        ### Pass Through Layers ###\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states,\n",
    "                                attention_mask,\n",
    "                                chunked_attention_mask if chunked_attention_mask is not None else causal_mask,\n",
    "                                past_key_values,\n",
    "                                cache_position,\n",
    "                                freq_cis)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return hidden_states, past_key_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 注意力的掩码张量计算包含了两个计算：一个是自回归的因果掩码，一个是分块掩码。\n",
    "    # 首先是因果掩码\n",
    "    def _prepare_4d_causal_attention_mask_with_cache_position(\n",
    "            self,\n",
    "            attention_mask,\n",
    "            sequence_length,\n",
    "            cache_position,\n",
    "            batch_size,\n",
    "            dtype,\n",
    "            device\n",
    "    ):\n",
    "        # 如果掩码张量已存在，则直接使用\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = attention_mask\n",
    "\n",
    "        else:\n",
    "\n",
    "            ### Create a Causal Mask ###\n",
    "            # torch.finfo(dtype).min是用来获取指定数据类型（dtype）的最小值\n",
    "            # 如：min_float32=-3.4028235e+38，而min_float64:-1.7976931348623157e+308\n",
    "            min_dtype = torch.finfo(dtype).min\n",
    "\n",
    "            # 创建一个形状为(seq*seq)大小的用最小值min_type填充的矩阵\n",
    "            # 为啥要用极小值？ 因为在做softmax的时候，如果用0填充，则softmax后，e^0 = 1，掩码的部分不会变小，而是获得更多的关注。\n",
    "            # 而使用极小值，会在经过softmax后，掩码的值更趋近于0\n",
    "            causal_mask = torch.full(\n",
    "                (sequence_length, sequence_length), fill_value=min_dtype, dtype=dtype, device=device\n",
    "            )\n",
    "\n",
    "            ### If we have more than one token, we need causal mask, so current token doesnt look forward ###\n",
    "            # 应用因果掩码，右上三角\n",
    "            if sequence_length != 1:\n",
    "                causal_mask = torch.triu(causal_mask, diagonal=1)\n",
    "\n",
    "            ### Check Wherever our Sequence index is longer than the cache positions ###\n",
    "            ### If a Query is BEFORE the Key/Value Cache Index, then the Query would have ###\n",
    "            ### to look into the future which is not allowed and needs to be masked! ###\n",
    "\n",
    "            # 作用，生成一个掩码，保证每个token只能看到自己之前的单词，没办法看到自己之后的单词。\n",
    "            # 这个代码的思路很有意思，生成seq_len长度的数，与位置缓存中的内容进行比对，返回True或False。\n",
    "            causal_mask *= torch.arange(sequence_length, device=device) > cache_position.to(device).reshape(-1,1)\n",
    "\n",
    "            ### Add Extra Dimension to causal Mask (4 dimensions, with placeholder for head dim) ###\n",
    "            # 将掩码张量添加batch_size，在第一维度，变为(batch_size, 1, sequence_length, sequence_length)\n",
    "            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n",
    "\n",
    "            ### If Attention mask is Not None, then add causal and attention mask together ###\n",
    "            if attention_mask is not None:\n",
    "                # 如果attention_mask不为空，则克隆一份作为causal_mask，原始causal_mask不被修改\n",
    "                causal_mask = causal_mask.clone()\n",
    "\n",
    "                ### Only keep upto length in attention mask ###\n",
    "                # 掩码的长度为最后一个维度\n",
    "                mask_length = attention_mask.shape[-1]\n",
    "                #causal_mask[:, :, :, :mask_length] 和 attention_mask[:, None, None, :] 进行了加法，生成了一个新的 padding_mask，它包含了 causal_mask 和 attention_mask 合并后的结果。\n",
    "                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(device)\n",
    "                # 生成bool值的掩码，当padding mask中的元素值为0的地方会被设置为True，否则为False\n",
    "                padding_mask = (padding_mask==0)\n",
    "                # masked_fill会将padding mask中为True的元素，使用极小值min dtype进行替换\n",
    "                causal_mask[:,:,:,:mask_length] = causal_mask[:,:,:,:mask_length].masked_fill(padding_mask, min_dtype)\n",
    "\n",
    "            return causal_mask\n",
    "    # 创建分块注意力掩码\n",
    "    def _create_chunked_attention_mask(\n",
    "            self,\n",
    "            attention_chunk_size,\n",
    "            start,\n",
    "            end,\n",
    "            device\n",
    "    ):\n",
    "\n",
    "        ### Create Blocks in the Chunk Sizes you want ###\n",
    "        # tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
    "        #         [0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
    "        #         [0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
    "        #         [1, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "        #         [1, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "        #         [1, 1, 1, 0, 0, 0, 1, 1, 1],\n",
    "        #         [2, 2, 2, 1, 1, 1, 0, 0, 0],\n",
    "        #         [2, 2, 2, 1, 1, 1, 0, 0, 0],\n",
    "        #         [2, 2, 2, 1, 1, 1, 0, 0, 0]])\n",
    "\n",
    "        # 按照块的大小attention_chunk_size进行创建掩码，创建两个长度相同的矩阵，分别在第1维度和第二维度添加新维度，分别与attention_chunk_size做整除，然后使用abs做两个张量之间差的绝对值\n",
    "        # 行向量和列向量通过广播机制进行相减，得到的结果为 (end-start)*(end-start)形状的矩阵\n",
    "        block_pos = torch.abs(\n",
    "            (torch.arange(start, end).unsqueeze(0)//attention_chunk_size) -\n",
    "            (torch.arange(start, end).unsqueeze(1)//attention_chunk_size)\n",
    "        )\n",
    "\n",
    "        ### Do computation again but without absolute value or division to get token positions###\n",
    "        # tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
    "        #         [-1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
    "        #         [-2, -1,  0,  1,  2,  3,  4,  5,  6],\n",
    "        #         [-3, -2, -1,  0,  1,  2,  3,  4,  5],\n",
    "        #         [-4, -3, -2, -1,  0,  1,  2,  3,  4],\n",
    "        #         [-5, -4, -3, -2, -1,  0,  1,  2,  3],\n",
    "        #         [-6, -5, -4, -3, -2, -1,  0,  1,  2],\n",
    "        #         [-7, -6, -5, -4, -3, -2, -1,  0,  1],\n",
    "        #         [-8, -7, -6, -5, -4, -3, -2, -1,  0]])\n",
    "\n",
    "        #构造一个二维的“位置差值矩阵”，行列相减，广播计算。\n",
    "        tok_pos = torch.arange(start, end).unsqueeze(0) - torch.arange(start,end).unsqueeze(1)\n",
    "\n",
    "        ### Wherever our block pos is 0 (our chunks) and our tok_pos is negative (causal mask) We want those ###\n",
    "        # 上面两个矩阵做运算，最终结果：\n",
    "        # tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                # [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                # [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "                # [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                # [0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "                # [0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "                # [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                # [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "                # [0, 0, 0, 0, 0, 0, 1, 1, 1]])\n",
    "\n",
    "        mask = (block_pos==0) & (tok_pos<=0)\n",
    "        # 此处计算，相当于既保留因果掩码，同时也使用分块注意力机制来减少计算量。\n",
    "        return mask.to(device)\n",
    "\n",
    "    def _update_causal_mask(self,\n",
    "                            attention_mask,\n",
    "                            input_tensor,\n",
    "                            cache_position):\n",
    "\n",
    "        ### Get Sequence Length and Attention Chunk Size ###\n",
    "        ### Honestly the attention chunk size is huge (over 8000)\n",
    "        ### we could probably skip this as we are just creating a\n",
    "        ### minimal version here but lets just do it anyway!\n",
    "\n",
    "        #序列长度\n",
    "        seq_len = input_tensor.shape[1]\n",
    "        # 获取分块注意力机制分多少块\n",
    "        attention_chunk_size = self.config.attention_chunk_size\n",
    "\n",
    "        ### Get the Starting and Ending Position from Cache ###\n",
    "        # 从位置信息缓存获取开始的token位置，默认batch_size为1\n",
    "        start_cache_pos = cache_position[0]\n",
    "\n",
    "        ### Two Checks: Is our First Cache Position greater than our chunk size:\n",
    "        # 确认当前是否超过attention_chunk_size，如果超过attention_chunk_size，代表不是第一个注意力掩码的分块，有可能是第二个或者更后面的chunk\n",
    "        cond1 = start_cache_pos >= attention_chunk_size\n",
    "\n",
    "        ### Is our first cache in the first chunk, but with the seq_len it rolls over to the second chunk ###\n",
    "        # 判断当前序列是否跨越了一个 attention chunk 的边界。这个条件的设计在分块注意力中非常关键，用于决定是否要做特殊的 mask 处理。\n",
    "        #\n",
    "        cond2 = (start_cache_pos < attention_chunk_size) & (start_cache_pos + seq_len > attention_chunk_size)\n",
    "\n",
    "        ### This is just a fancy if/else\n",
    "        ### If cond1 is True then key_length = attention_chunk_size + seq_len - 1\n",
    "        ### elif cond1 is false, then if cond2 is True then key_length = start_cache_pos + seq_len\n",
    "        ### else key_length = attention_chunk_size\n",
    "\n",
    "        # 当cond1 == True，表示整个序列起始位置就在 chunk 边界 之后，即你已经处在 chunk 1、chunk 2 等之后的块中。key 长度设为attention_chunk_size + seq_len - 1，key 范围涵盖前一个 chunk 全部，加上当前序列的长度，-1是为了对其index\n",
    "        # 当cond1 == False 且 cond2 == True，起始位置在第一个 chunk，但本次序列 跨越 chunk 边界。start_cache_pos + seq_len。表示从起始位置向前看，只允许看到从头到当前序列末尾的所有位置。\n",
    "        # 当cond1 == False 且 cond2 == False，key的长度为attention_chunk_size，表示 attention mask 只允许关注第一个 chunk 内的 token。\n",
    "        key_length = torch.where(\n",
    "            cond1,\n",
    "            attention_chunk_size + seq_len - 1,\n",
    "            torch.where(cond2, start_cache_pos + seq_len, attention_chunk_size),\n",
    "        )\n",
    "\n",
    "        # 对于短序列，采用完整的因果注意力机制掩码。\n",
    "        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n",
    "            attention_mask,\n",
    "            sequence_length=seq_len,\n",
    "            cache_position=cache_position,\n",
    "            batch_size=input_tensor.shape[0],\n",
    "            dtype=input_tensor.dtype,\n",
    "            device=input_tensor.device\n",
    "        )\n",
    "\n",
    "        # 对于长序列，使用分块注意力机制掩码，同时需要注意keylength的条件\n",
    "        chunked_attention_mask = None\n",
    "        if seq_len > self.config.attention_chunk_size:\n",
    "            chunked_attention_mask = self._create_chunked_attention_mask(\n",
    "                self.config.attention_chunk_size,\n",
    "                start=start_cache_pos,\n",
    "                end=start_cache_pos+key_length,\n",
    "                device=input_tensor.device\n",
    "            )\n",
    "\n",
    "            ### Mask only valid wherever attention mask is valid ###\n",
    "            # 避免 padding 位置参与 attention。\n",
    "            # 即使 chunked 允许 attend 某些位置，如果 attention_mask 说这些是 padding，也强行掩掉。\n",
    "            chunked_attention_mask = chunked_attention_mask & attention_mask\n",
    "\n",
    "            ### Add dimensions and fill with -inf ###\n",
    "\n",
    "            # 最终结果chunked_attention_mask是形状 [1, 1, T_q, T_k]、类型为 float16/32 的张量\n",
    "            chunked_attention_mask = (\n",
    "                    # 将 chunked_attention_mask的shape 从 [T_q, T_k] 扩展为 [1, 1, T_q, T_k]\n",
    "                    chunked_attention_mask[None, None, :, :]\n",
    "                    .to(input_tensor.dtype)\n",
    "                    # 掩码填充最小值，如果chunked_attention_mask元素为True则填充float的极小值作为掩码\n",
    "                    .masked_fill(chunked_attention_mask, torch.finfo(input_tensor.dtype).min)\n",
    "                )\n",
    "\n",
    "        return causal_mask, chunked_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731,
     "referenced_widgets": [
      "62ba97d31ddf469792c20fdee2b7b629",
      "043e40d605054ef0a182d844c6476675",
      "596a133159fd4c6685fbbc8024cb9f23",
      "5d96e5142162408bbcc5ec6fab2ab954",
      "6640c1e6aaa64dbaabdaef7f79e87ba1",
      "b49f50e3e10f46b5a0b55550b18e5654",
      "01c4076452974ff8ac8b87a06921f265",
      "809d29e1b15948b894a5c1844db71696",
      "e56ffdd174e64d80b0084b69388e01c3",
      "df034abf115c4b5a9bd9ac09c327c0b7",
      "e797541c2dd144259e4e57fc70c7c44e",
      "1e44e668d5cd43278d11483d59d0c239",
      "e8f7bb50933b4a0c848cec41801f92da",
      "6513bcfe0ec24a78807ab2c39e9d7831",
      "0252c3f162344bce93d23578f1f0a9c8",
      "28ade5519eca4d07b2726e9558b1541a",
      "0a1a5cf0e18941f28bc027128eb78ae0",
      "9214d124496940448005896ec41fa847",
      "e6d1ec41c5bb49ca9473f8f39977e638",
      "42c958c345be40fc901da51abcd70fec",
      "5e9b0fe2580d4c5db0a18f8d976e5b14",
      "b46cceb378b148a3967b435ba7f7c873"
     ]
    },
    "executionInfo": {
     "elapsed": 752225,
     "status": "ok",
     "timestamp": 1746494199511,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "b885acdd",
    "outputId": "d8f8a522-889d-4991-c533-02950b9645e9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 初始化模型组件\n",
    "embedding = nn.Embedding(config_test.vocab_size, config_test.hidden_size).to(device)\n",
    "model = Llama4TextModel(config_test).to(device)\n",
    "output_head = nn.Linear(config_test.hidden_size, config_test.vocab_size).to(device)\n",
    "\n",
    "# 损失函数与优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    list(model.parameters()) + list(embedding.parameters()) + list(output_head.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# 超参数\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "context_window = 128\n",
    "log_every = 100  # 每 log_every 个 batch 记录一次损失\n",
    "\n",
    "train_loader = create_dataloader(dataset, batch_size, context_window, split='train')\n",
    "\n",
    "# 存储每次记录时的 batch 编号和对应的 loss\n",
    "logged_batches = []\n",
    "logged_losses = []\n",
    "\n",
    "# ==== 训练循环 ====\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(f\"\\n Epoch {epoch+1}/{epochs}\")\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch_idx, (inputs,) in enumerate(progress_bar):\n",
    "        inputs = inputs.long().to(device)\n",
    "        targets = inputs[:, 1:].contiguous()\n",
    "        inputs = inputs[:, :-1].contiguous()\n",
    "\n",
    "        input_embeds = embedding(inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_embeds=input_embeds)\n",
    "        hidden_states = outputs[0]\n",
    "        logits = output_head(hidden_states)\n",
    "\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = loss_fn(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 控制记录频率\n",
    "        if (batch_idx + 1) % log_every == 0:\n",
    "            avg_batch_loss = total_loss / (batch_idx + 1)\n",
    "            logged_batches.append(batch_idx + 1)\n",
    "            logged_losses.append(avg_batch_loss)\n",
    "\n",
    "        # tqdm 实时更新\n",
    "        progress_bar.set_postfix({\n",
    "            \"Batch Loss\": f\"{loss.item():.4f}\",\n",
    "            \"Avg Loss\": f\"{total_loss / (batch_idx + 1):.4f}\"\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1} finished | 🔺 Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ==== 绘图 ====\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(logged_batches, logged_losses, marker='o', linestyle='-', color='blue', label='Avg Loss')\n",
    "plt.title('Loss Curve During Training', fontsize=16)\n",
    "plt.xlabel('Batch Index', fontsize=12)\n",
    "plt.ylabel('Average Loss', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_curve_smooth.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1746492233119,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "243c4d7e",
    "outputId": "7c2bc702-5bd4-47a2-d18d-b6015408158e"
   },
   "outputs": [],
   "source": [
    "save_path = \"./llama4text_model.pth\"\n",
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"embedding_state\": embedding.state_dict(),\n",
    "    \"output_head_state\": output_head.state_dict(),\n",
    "    \"vocab\": vocab,\n",
    "    \"config\": config_test.__dict__\n",
    "}, save_path)\n",
    "print(\"模型已保存！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 164,
     "status": "ok",
     "timestamp": 1746492233284,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "020f20a1",
    "outputId": "8cb5e2ad-65f1-4265-fbb9-c6e0b579e474"
   },
   "outputs": [],
   "source": [
    "print(\"正在加载模型权重...\")\n",
    "checkpoint = torch.load(\"llama4text_model.pth\", map_location=device)\n",
    "\n",
    "vocab = checkpoint['vocab']\n",
    "itos = {i: ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
    "\n",
    "config_test_generate = Llama4TextConfig(**checkpoint[\"config\"])\n",
    "embedding = nn.Embedding(config_test_generate.vocab_size, config_test_generate.hidden_size).to(device)\n",
    "model = Llama4TextModel(config_test_generate).to(device)\n",
    "output_head = nn.Linear(config_test_generate.hidden_size, config_test_generate.vocab_size).to(device)\n",
    "\n",
    "embedding.load_state_dict(checkpoint['embedding_state'])\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "output_head.load_state_dict(checkpoint['output_head_state'])\n",
    "\n",
    "print(\"模型加载完毕咯\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114,
     "referenced_widgets": [
      "ac5ad658f69f4592b95abf7e140c34cd",
      "91b2f9ca65b140e1bec6dd12abbfe2ae",
      "1f9f72ce5b3c4c62835ae17a5b48ac92",
      "273ac28160a740efb61e0d30c6a00778",
      "84c0541530ef4f009ac9238a3990f567",
      "c4d7861f60194cbeb946c4ce4f22bf61",
      "9ee1140c14094c10bc052386239c6046",
      "636ca9d535c94c3bbb1d8b60ae31ca8c",
      "f4a62dcddca54f15b22713850489531d",
      "0c32000458ad46e88030f0cfa7b156bf",
      "6543268fa48c45eca7bc4367ba948f57"
     ]
    },
    "executionInfo": {
     "elapsed": 1247,
     "status": "ok",
     "timestamp": 1746494348957,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "81972f4b",
    "outputId": "05bd89d1-8a66-46e0-f793-23d6da0feec3"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def sample_next_token(logits, top_k=10, top_p=0.9, temperature=1.0):\n",
    "    logits = logits / temperature\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Top-k 筛选\n",
    "    if top_k is not None:\n",
    "        top_k = min(top_k, probs.size(-1))\n",
    "        values, indices = torch.topk(probs, top_k)\n",
    "        probs_zero = torch.zeros_like(probs).scatter(-1, indices, values)\n",
    "        probs = probs_zero / probs_zero.sum()\n",
    "\n",
    "    # Top-p 筛选\n",
    "    if top_p is not None:\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        cutoff = cumulative_probs > top_p\n",
    "        if torch.any(cutoff):\n",
    "            last_index = torch.where(cutoff)[0][0] + 1\n",
    "            sorted_probs = sorted_probs[:last_index]\n",
    "            sorted_indices = sorted_indices[:last_index]\n",
    "        probs = torch.zeros_like(probs).scatter(-1, sorted_indices, sorted_probs)\n",
    "        probs = probs / probs.sum()\n",
    "\n",
    "    return torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "def generate_text(start_text='', length=200, top_k=10, top_p=0.9, temperature=1.0):\n",
    "    if not start_text:\n",
    "        start_text = random.choice(vocab)\n",
    "    print(f\"🌱 初始字符: {start_text}\")\n",
    "\n",
    "    input_ids = torch.tensor([stoi[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in tqdm(range(length), desc=\"Generating\"):\n",
    "        input_embeds = embedding(input_ids)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_embeds=input_embeds)\n",
    "        hidden_states = outputs[0]\n",
    "        logits = output_head(hidden_states[:, -1, :])  # 只取最后一个 token 的 logits\n",
    "\n",
    "        next_id = sample_next_token(logits.squeeze(), top_k=top_k, top_p=top_p, temperature=temperature)\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[next_id]], device=device)], dim=1)\n",
    "\n",
    "    generated = decode(input_ids[0].tolist())\n",
    "    return generated\n",
    "\n",
    "\n",
    "generated = generate_text(length=100, top_k=3, top_p=0.9, temperature=1.7)\n",
    "display(Markdown(generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746494345536,
     "user": {
      "displayName": "chi ma",
      "userId": "09061819115802133413"
     },
     "user_tz": -480
    },
    "id": "555f1b3c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
